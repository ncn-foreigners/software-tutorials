---
title: "Short software tutorial on `nonprobsvy` package"
author: Maciej BerÄ™sewicz
format: 
  html:
    number-sections: true
    self-contained: true
    toc: true
    df-print: kable
lang: en
bibliography: references.bib
biblio-style: apalike
execute: 
  warning: false
---

# Basic information

This tutorial shows basic usage of the
[nonprobsvy](https://github.com/ncn-foreigners/nonprobsvy) package
developed in this project. This package implements mass imputation,
inverse probability weighting and doubly robust estimators based on the
following papers:

-   Yang, S., Kim, J. K., & Song, R. (2020). Doubly robust inference
    when combining probability and non-probability samples with high
    dimensional data. Journal of the Royal Statistical Society. Series
    B, Statistical Methodology, 82(2), 445.
-   Kim, J. K., Park, S., Chen, Y., & Wu, C. (2021). Combining
    non-probability and probability survey samples through mass
    imputation. Journal of the Royal Statistical Society Series A:
    Statistics in Society, 184(3), 941-963.
-   Chen, Y., Li, P., & Wu, C. (2020). Doubly robust inference with
    nonprobability survey samples. Journal of the American Statistical
    Association, 115(532), 2011-2021.

Tutorial was prepared based on the development version of the
`nonprobsvy` (21.06.2023, commit
`5ab78e476555ac527f9387d2f4c67637b7b38650`).

# Install and load the required packages

Install `remotes` package and then install `nonprobsvy` package from
github repository.

```{r install-packages, eval=FALSE}
install.packages("remotes")
remotes::install_github("ncn-foreigners/nonprobsvy@5ab78e476555ac527f9387d2f4c67637b7b38650")
install.packages("survey")
install.packages("sampling")
```

Load required packages

```{r load-packages}
library(survey)
library(sampling)
library(sps)
library(nonprobsvy)
library(data.table)
library(Rcpp)
```

Function for randomized systematic sampling based on prof. Changbao Wu's
[webpage](https://sas.uwaterloo.ca/~cbwu/Rcodes/SystematicPPS.txt). 

```{r}
syspps <- function(x,n){ 
  N <- length(x)
  U <- sample(N, N)
  xx <- x[U]
  z <- rep(0, N)
  for (i in 1:N) z[i] <- n * sum(xx[1:i]) / sum(x)
  r <- runif(1)
  s <-  numeric(n)
  j <- 1
  for (i in 1:N) {
    if (z[i] >= r) {
      s[j] <- U[i]
      r <- r + 1
      j <- j + 1
    }
  }
  return(s[order(s)])
}
```

One can also use an C++ version of this function available in the `codes/syspps.cpp` file.

```{r}
sourceCpp("codes/syspps.cpp")
```


In the empirical part we reproduce results from the following papers:
@yang2020; @kim2021; @chen2020.

Seed for all simulations

```{r}
seed_for_sim <- 20230621
```

# Empirical examples

## @yang2020 paper

### Simulation from section 6

> In this section, we evaluate the finite sample performance of the
> procedure proposed. We first generate a finite population
> $\mathcal{F}_{N}=\left\{(X_{i}, Y_{i}): i=1, \ldots, N\right\}$ with
> $N=10000$, where $Y_{i}$ is a continuous or binary outcome variable,
> and $X_{i}=(1, X_{1, i}, \ldots, X_{p-1, i})^{\mathrm{T}}$ is a
> $p$-dimensional vector of covariates with the first component being 1
> and other components independently generated from the standard normal
> distribution. We set $p=50$. From the finite population, we select a
> non-probability sample $\mathcal{B}$ of size
> $n_{\mathrm{B}} \approx 2000$, according to the selection indicator
> $I_{\mathrm{B}, i} \sim \operatorname{Ber}(\pi_{\mathrm{B}, i})$.

> We select a probability sample $\mathcal{A}$ of the average size
> $n_{\mathrm{A}}=500$ under Poisson sampling with
> $\pi_{\mathrm{A}, i} \propto(0.25+\left|X_{1 i}\right|+0.03\left|Y_{i}\right|)$.
> The parameter of interest is the population mean
> $\mu=N^{-1} \sum_{i=1}^{N} Y_{i}$.

> For the non-probability sampling probability, we consider both linear
> and non-linear sam- pling score models:

> -   $\operatorname{logit}(\pi_{\mathrm{B}, i})=\alpha_{0}^{\mathrm{T}} X_{i}$,
>     where $\alpha_{0}=(-2,1,1,1,1,0,0,0, \ldots, 0)^{\mathrm{T}}$
>     (model PSM I)
> -   $\operatorname{logit}(\pi_{\mathrm{B}, i})=3.5+\alpha_{0}^{\mathrm{T}} \log (X_{i}^{2})-\sin (X_{3, i}+X_{4, i})-X_{5, i}-X_{6, i}$,
>     where $\alpha_{0}=(0,0,0,3,3,3,3$ $0, \ldots, 0)^{\mathrm{T}}$
>     (model PSM II).

For generating a continuous outcome variable $Y_{i}$, we consider both
linear and non-linear outcome models with
$\beta_{0}=(1,0,0,1,1,1,1,0, \ldots, 0)^{\mathrm{T}}$ :

-   

    (a) $Y_{i}=\beta_{0}^{\mathrm{T}} X_{i}+\epsilon_{i}, \epsilon_{i} \sim \mathcal{N}(0,1)$
        (model OM I);

-   

    (b) $Y_{i}=1+\exp \left\{3 \sin (\beta_{0}^{\mathrm{T}} X_{i})\right\}+X_{5, i}+X_{6, i}+\epsilon_{i}, \epsilon_{i} \sim \mathcal{N}(0,1)$
        (model OM II).

For generating a binary outcome variable $Y_{i}$, we consider both
linear and non-linear outcome models with
$\beta_{0}=(1,0,0,3,3,3,3,0, \ldots, 0)^{\mathrm{T}}$,

-   

    (a) $Y \sim \operatorname{Ber}\left\{\pi_{Y}(X)\right\}$ with logit
        $\left\{\pi_{Y}(X)\right\}=\beta_{0}^{\mathrm{T}} X$ (model OM
        III);

-   

    (b) $Y \sim \operatorname{Ber}\left\{\pi_{Y}(X)\right\}$ with logit
        $\left\{\pi_{Y}(X)\right\}=2-\log \left\{(\beta_{0}^{\mathrm{T}} X)^{2}\right\}+2 X_{5, i}+2 X_{6, i}$
        (model OM IV).

We consider the following estimators

-   

    (a) naive, $\hat{\mu}_{\text {naive }}$, the naive estimator using
        the simple average of $Y_{i}$ from sample $\mathrm{B}$, which
        provides the degree of the selection bias of sample
        $\mathrm{B}$;

-   

    (b) oracle, $\hat{\mu}_{\text {ora }}$, the doubly robust estimator
        $\hat{\mu}_{\mathrm{dr}}(\hat{\alpha}_{\text {ora }}, \hat{\beta}_{\text {ora }})$,
        where $\hat{\alpha}_{\text {ora }}$ and
        $\hat{\beta}_{\text {ora }}$ are based on the joint estimator
        restricting to the known important covariates for comparison;

-   

    (c) $p-ipw$, $\hat{\mu}_{\mathrm{p} \text {-ipw }}$, the penalized
        inverse probability of sampling weighting estimator
        $\hat{\mu}_{\mathrm{IPW}}=$
        $N^{-1} \Sigma_{i \in \mathcal{B}} \hat{\pi}_{\mathrm{B}, i}^{-1} Y_{i}$,
        where logit
        $(\hat{\pi}_{\mathrm{B}, i})=X_{i}^{\mathrm{T}} \hat{\alpha}$
        using a logistic regression model, and $\hat{\alpha}$ is
        obtained by a weighted penalized regression of
        $I_{\mathrm{B}, i}$ on $X_{i}$ based on the combined data from
        sample $\mathrm{A}$ and sample $\mathrm{B}$, with the units in
        sample A weighted by the known sampling weights and the units in
        sample $\mathrm{B}$ weighted by 1 .

-   

    (d) $p-reg$, $\hat{\mu}_{\mathrm{p} \text {-reg }}$, the penalized
        regression estimator
        $\hat{\mu}_{\mathrm{p}-\mathrm{reg}}=N^{-1} \Sigma_{i \in \mathcal{A}} d_{\mathrm{A}, i} m(X ; \hat{\beta})$,
        where $\hat{\beta}$ is obtained by a penalized regression of
        $Y_{i}$ on $X_{i}$ based on sample $\mathrm{B}$;

-   

    (e) $p-dr0$, $\hat{\mu}_{\mathrm{p} \text {-dr } 0}$, the penalized
        double estimating equation estimator based on the set of outcome
        predictors $\hat{\mathcal{M}}_{\beta}$;

-   

    (f) $p-dr$, $\hat{\mu}_{\mathrm{p}-\mathrm{dr}}$, the proposed
        penalized double estimating equation estimator based on the
        union of sampling and outcome predictors
        $\hat{\mathcal{M}}_{\alpha} \cup \hat{\mathcal{M}}_{\beta}$.

We also note that $\hat{\mu}_{\mathrm{dr}}$ without variable selection
is severely biased and unstable and therefore is excluded for
comparison.

## @chen2020 paper

### Simulation from section 5

#### Description of the simulation

The simulation is based on the following assumptions:

+ population size $N=20,000$
+ probability sample $B$ is of size $500$ or $1000$
+ non-probability sample $A$ is of size $500$ or $1000$
+ auxiliary variables $x$ are generated as follows:
  + $z_{1 i} \sim \operatorname{Bernoulli}(0.5)$
  + $z_{2 i} \sim \operatorname{Uniform}(0,2)$
  + $z_{3 i} \sim \operatorname{Exponential}(1)$
  + $z_{4 i} \sim \chi^{2}(4)$
  + $x_{1 i}=z_{1i}$
  + $x_{2 i}=z_{2 i}+0.3 x_{1 i}$
  + $x_{3 i}=z_{3 i}+0.2(x_{1 i}+x_{2 i})$
  + $x_{4 i}=z_{4 i}+0.1(x_{1 i}+x_{2 i}+x_{3 i})$
+ target varible $y$ is generated using the following linear model

$$
y_{i}=2+x_{1 i}+x_{2 i}+x_{3 i}+x_{4 i}+\sigma \varepsilon_{i}, \quad i=1,2, \ldots, N
$$

where $\varepsilon_{i}$ 's are independent and identically
distributed (iid) as $N(0,1)$,

+ values of $\sigma$ chosen such that the correlation coefficient $\rho$ between $y$ and the linear predictor $\boldsymbol{x}^{\top} \boldsymbol{\beta}$ is controlled at $0.3, 0.5$ and $0.8$
+ The true propensity scores
$\pi_{i}^A$ for the non-probability sample $A$ follow the logistic regression model: 

$$
\log (\frac{\pi_{i}^{\mathrm{A}}}{1-\pi_{i}^{\mathrm{A}}})=\theta_{0}+0.1 x_{1 i}+0.2 x_{2 i}+0.1 x_{3 i}+0.2 x_{4 i}
$$

where $\theta_{0}$ is chosen such that $\sum_{i=1}^{N} \pi_{i}^{\mathrm{A}}=n_{\mathrm{A}}$ with the given
target sample size $n_{\mathrm{A}}$. 
 
+ The nonprobability sample $A$ is selected by the Poisson sampling method with inclusion probabilities specified by $\pi_{i}^A$ and the target sample size $n_A$.

+ The probability sample $B$ with the target size $n_B$ is taken by the randomized systematic PPS sampling method with the inclusion probabilities $\pi_{i}^B$ proportional to $z_{i}=c+x_{3 i}$. The value of $c$ is chosen to control the variation of the survey weights such that $\max z_{i} / \min z_{i}=50$.

+ Four scenarios are considered:
  + Scenario 1 (denoted TT): both models are specified correctly,
  + Scenario 2 (FT): model for $y$ is missing $x_4$ so it is wrongly specified,
  + Scenario 3 (TF): selection is missing $x_4$ so it is wrongly speficied,
  + Scenario 4 (FF): both models miss $x_4$ so are wrongly specified.


#### Implementation

Simulate data

```{r}
set.seed(seed_for_sim)

N <- 20000
n_1 <- 500
n_2 <- 1000
z1 <- rbinom(N, 1, 0.5)
z2 <- runif(N,0,2)
z3 <- rexp(N,1)
z4 <- rchisq(N, 4)
x1 <- z1
x2 <- z2 + 0.3*z1
x3 <- z3 + 0.2*(x1 + x2)
x4 <- z4 + 0.1*(x1 + x2 + x3)
e <- rnorm(N)

etol <- 1e-8
sigma03 <- stats::uniroot(f = function(s) cor(2+x1+x2+x3+x4, 2+x1+x2+x3+x4+s*e) - 0.3, c(0,20), tol = etol)$root
sigma05 <- stats::uniroot(f = function(s) cor(2+x1+x2+x3+x4, 2+x1+x2+x3+x4+s*e) - 0.5, c(0,20), tol = etol)$root
sigma08 <- stats::uniroot(f = function(s) cor(2+x1+x2+x3+x4, 2+x1+x2+x3+x4+s*e) - 0.8, c(0,20), tol = etol)$root

y1 <- 2+x1+x2+x3+x4+sigma03*e
y2 <- 2+x1+x2+x3+x4+sigma05*e
y3 <- 2+x1+x2+x3+x4+sigma08*e

theta_A1 <- stats::uniroot(f = function(s) sum(plogis(s + 0.1*x1+0.2*x2+0.3*x3+0.2*x4))-n_1, c(-20,0), tol = etol)$root
theta_A2 <- stats::uniroot(f = function(s) sum(plogis(s + 0.1*x1+0.2*x2+0.3*x3+0.2*x4))-n_2, c(-20,0), tol = etol)$root

pA_1 <- plogis(theta_A1 + 0.1*x1+0.2*x2+0.3*x3+0.2*x4)
w_pA_1 <- 1/pA_1
pA_2 <- plogis(theta_A2 + 0.1*x1+0.2*x2+0.3*x3+0.2*x4)
w_pA_2 <- 1/pA_2

s_B <- stats::uniroot(f = function(s) max(s+x3)/min(s+x3) - 50, c(0, 100), tol = etol)$root
pB_1 <- inclusionprobabilities(s_B + x3, n_1)
pB_2 <- inclusionprobabilities(s_B + x3, n_2)
w_pB_1 <- 1/pB_1
w_pB_2 <- 1/pB_2

pop_data <- data.frame(x1,x2,x3,x4,y1,y2,y3,pA_1,pA_2,w_pA_1,w_pA_2,pB_1,pB_2,w_pB_1,w_pB_2)
head(pop_data)
```

One simulation

```{r}
set.seed(seed_for_sim)
sample_nonprob <- pop_data[which(sampling::UPpoisson(pop_data$pA_1)==1),]
sample_nonprob$w_naive <- N/n_1
sample_prob <- pop_data[syspps_cpp(pop_data$pB_1, n_1), ]
sample_prob_svy <- svydesign(ids=~1, probs =~pB_1, data = sample_prob)

res_s1 <- nonprob(selection = ~ x1 + x2 + x3 + x4,
                  outcome = y1 ~ x1 + x2 + x3 + x4,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s2 <- nonprob(selection = ~ x1 + x2 + x3 + x4,
                  outcome = y1 ~ x1 + x2 + x3,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s3 <- nonprob(selection = ~ x1 + x2 + x3 ,
                  outcome = y1 ~ x1 + x2 + x3 + x4,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s4 <- nonprob(selection = ~ x1 + x2 + x3 ,
                  outcome = y1 ~ x1 + x2 + x3 ,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

cbind(
  lapply(list(res_s1,res_s2,res_s3,res_s4), "[[", "output") |> rbindlist(),
  lapply(list(res_s1,res_s2,res_s3,res_s4), "[[", "confidence_interval") |> rbindlist()
) 
```

Whole simulation using $B=5000$ as in the original paper

```{r sim-chen2020-5000, eval = F}
set.seed(seed_for_sim)
sim_y1 <- list()
B <- 500

for (b in 1:B) {
  if (b %% 500 == 0) print(b)
  sample_nonprob <- pop_data[which(sampling::UPpoisson(pop_data$pA_1)==1),]
  sample_nonprob$w_naive <- N/n_1
  sample_prob <- pop_data[syspps_cpp(pop_data$pB_1, n_2), ]
  sample_prob_svy <- svydesign(ids=~1, probs =~pB_1, data = sample_prob)

res_s1 <- nonprob(selection = ~ x1 + x2 + x3 + x4,
                  outcome = y1 ~ x1 + x2 + x3 + x4,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s2 <- nonprob(selection = ~ x1 + x2 + x3 + x4,
                  outcome = y1 ~ x1 + x2 + x3,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s3 <- nonprob(selection = ~ x1 + x2 + x3 ,
                  outcome = y1 ~ x1 + x2 + x3 + x4,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

res_s4 <- nonprob(selection = ~ x1 + x2 + x3 ,
                  outcome = y1 ~ x1 + x2 + x3 ,
                  svydesign = sample_prob_svy,
                  data = sample_nonprob)

sim_y1[[b]] <- cbind(lapply(list(res_s1,res_s2,res_s3,res_s4), "[[", "output") |> rbindlist(),
                     lapply(list(res_s1,res_s2,res_s3,res_s4), "[[", "confidence_interval") |> rbindlist()) 
}
```


```{r sim-chen2020-5000-results, eval = F}
sim_y1_all <- rbindlist(sim_y1) |> na.omit()
sim_y1_all[, sim := rep(1:4, times = 4999)]

sim_y1_all[, .(bias=(mean(mean)- mean(y1))/mean(y1)*100, var = var(mean)), sim][, mse:=bias^2+var][]

sim_y1_all[, .(ci=mean(lower_bound < mean(y1) & mean(y1) < upper_bound)), sim]
```

## @kim2021 paper

### Simulation from section 6

#### Description of the simulation

The simulation is based on the following assumptions:

-   population size $N=100,000$,
-   probability sample $A$ of $500$ size,
-   non-probability sample $B$ of size $500$ and $1000$,
-   $x_{i} \sim N(2,1)$ and $e_{i} \sim (0,1)$,
-   the study variable $y_{i}$ is constructed differently for each
    model:
    -   Model I -- $y_{i}=1+2 x_{i}+e_{i}$ ($R^2=0.8$),
    -   Model II -- $y_{i}=3+x_{i}+2e_{i}$ ($R^2=0.2$),
    -   Model III -- $y_{i}=2.5 + 0.5x_{i}^2+e_{i}$,
-   SRS to obtain $A$ from each of three populations,
-   Sample $B$ is obtained by SRS within two strata:
    -   Strata 1: $x_i <= 2$ with size $n_1=0.7n_b$,
    -   Strata 2: $x_i > 2$ with size $n_2 = 0.3n_b$
-   Using the two samples $A$ and $B$, for estimators of
    $\theta_{N}=N^{-1} \sum_{i=1}^{N} y_{i}$ are computed:
    -   The sample mean from sample $A$ :
        $\widehat{\theta}_{A}=n_{A}^{-1} \sum_{i \in A} y_{i}$,
    -   The naive estimator (sample mean) from sample
        $B: \widehat{\theta}_{B}=n_{B}^{-1} \sum_{i \in B} y_{i}$.
    -   The mass imputation estimator from sample $A$ given in
        Equation (6) using
        $\widehat{y}_{i}=\widehat{\beta}_{0}+\widehat{\beta}_{1} x_{i}$
        where $(\widehat{\beta}_{0}, \widehat{\boldsymbol{\beta}}_{1})$
        are the estimated regression coefficients obtained from sample
        $B$.
    -   The IPW estimator proposed by @chen2020:
        $\widehat{\theta}_{I P W}=N^{-1} \sum_{i \in B} \widehat{\pi}_{i}^{-1} y_{i}$,
        where the propensity scores,
        $\pi_{i}=\pi(\mathbf{x}_{i} ; \boldsymbol{\phi})=\left\{1+\exp (-\phi_{0}-\phi_{1} x_{i})\right\}^{-1}$
        with $\mathbf{x}_{i}=(1, x_{i})^{\prime}$ and
        $\boldsymbol{\phi}=(\phi_{0}, \phi_{1})^{\prime}$, are estimated
        by using $\widehat{\boldsymbol{\phi}}$ which solves the
        following score equations: $$
        U(\boldsymbol{\phi})=\sum_{i \in B} \mathbf{x}_{i}-\sum_{i \in A} w_{i} \pi(\mathbf{x}_{i} ; \boldsymbol{\phi}) \mathbf{x}_{i}=\mathbf{0}
        $$

#### Implementation

```{r mi-sim-data}
set.seed(seed_for_sim)
N <- 100000
n_a <- 500
n_b <- 1000
n_b1 <- 0.7*n_b
n_b2 <- 0.3*n_b
x <- rnorm(N, 2, 1)
e <- rnorm(N)
y1 <- 1 + 2*x + e
y2 <- 3 + x + 2*e
y3 <- 2.5 + 0.5*x^2 + e
strata <- x <= 2
pop <- data.frame(x, y1, y2, y3, strata)
sample_a <- pop[sample(1:N, n_a),]
sample_a$w_a <- N/n_a
svy_a <- svydesign(ids= ~1, weights = ~ w_a, data = sample_a)
svy_a_cal <- calibrate(svy_a, formula=~ x, population=c(`(Intercept)`=N, x = sum(x)))
pop1 <- subset(pop, strata == TRUE)
pop2 <- subset(pop, strata == FALSE)
sample_b <- rbind(pop1[sample(1:nrow(pop1), n_b1), ],
                  pop2[sample(1:nrow(pop2), n_b2), ])
sample_b$w_b <- N/n_b

## mass imputation
res_y1 <- nonprob(outcome = y1 ~ x,  data = sample_b, weights = sample_b$w_b, svydesign = svy_a)
res_y2 <- nonprob(outcome= y2 ~ x,  data = sample_b, weights = sample_b$w_b, svydesign = svy_a)
res_y3 <- nonprob(outcome= y3 ~ x,  data = sample_b, weights = sample_b$w_b, svydesign = svy_a)


data.frame(est = c("mi_y1","mi_y2", "mi_y3"),
           true = c(mean(y1), mean(y2), mean(y3)),
           naive_B = c(mean(sample_b$y1),mean(sample_b$y2), mean(sample_b$y3)),
           mi = c(res_y1$output$mean, res_y2$output$mean, res_y3$output$mean))
```

Run the simulation and compare results with the paper

```{r mi-simulation-1000}
#| cache: true
set.seed(20230602)
B <- 1000
results <- matrix(data = 0, nrow = B, ncol = 9)
results_ci <- matrix(data = 0, nrow = B, ncol = 6)
for (b in 1:B) {
  sample_a <- pop[sample(1:N, n_a),]
  sample_a$w_a <- N/n_a
  svy_a <- svydesign(ids= ~1, weights = ~ w_a, data = sample_a)
  pop1 <- subset(pop, strata == TRUE)
  pop2 <- subset(pop, strata == FALSE)
  sample_b <- rbind(pop1[sample(1:nrow(pop1), n_b1), ],
                    pop2[sample(1:nrow(pop2), n_b2), ])
  sample_b$w_b <- N/n_b
  res_y1 <- nonprob(outcome = y1 ~ x, data = sample_b, weights = sample_b$w_b, svydesign = svy_a)
  res_y2 <- nonprob(outcome = y2 ~ x, data = sample_b, weights = sample_b$w_b, svydesign = svy_a)
  res_y3 <- nonprob(outcome = y3 ~ x, data = sample_b, weights = sample_b$w_b, svydesign = svy_a)
  results[b, 1] <- res_y1$output$mean
  results[b, 2] <- res_y2$output$mean
  results[b, 3] <- res_y3$output$mean
  results[b, 4] <- mean(sample_a$y1)
  results[b, 5] <- mean(sample_a$y2)
  results[b, 6] <- mean(sample_a$y3)
  results[b, 7] <- mean(sample_b$y1)
  results[b, 8] <- mean(sample_b$y2)
  results[b, 9] <- mean(sample_b$y3)
  
  results_ci[b, 1] <- res_y1$confidence_interval$lower_bound
  results_ci[b, 2] <- res_y1$confidence_interval$upper_bound
  results_ci[b, 3] <- res_y2$confidence_interval$lower_bound
  results_ci[b, 4] <- res_y2$confidence_interval$upper_bound
  results_ci[b, 5] <- res_y3$confidence_interval$lower_bound
  results_ci[b, 6] <- res_y3$confidence_interval$upper_bound
}
```

Bias, variance, mse and relative MSE calculated as

$$
RelMSE = \frac{MSE(\hat{\theta})}{MSE(\hat{\theta}_A)},
$$ where $\theta_A$ is the target parameter calculated if random sample
would be available for $y1,y2$ and $y3$.

```{r mi-sim-bias-reports}
results_mi <- data.frame(
  est = c("mi_y1", "mi_y2", "mi_y3", "naiveA_y1", "naiveA_y2", "naiveA_y3",
          "naiveB_y1", "naiveB_y2", "naiveB_y3"),
  bias = colMeans(results) - c(colMeans(pop[, 2:4]), colMeans(pop[, 2:4]), colMeans(pop[, 2:4])),
  var = apply(results, 2, var)
) 

results_mi$mse <- with(results_mi, bias^2+var)
results_mi$relMSE <- results_mi$mse/results_mi$mse[c(4,5,6,4,5,6,4,5,6)]*100
results_mi[c(4:nrow(results_mi), 1:3),]
```

We can compare the results with table 3 from @kim2021

![](figs/jrssa-mi-paper-tab3.png){fig-align="center" width="700"}

Check the coverage of confidence interval

```{r mi-sim-ci-report}
c("mi_y1"=mean(results_ci[, 1] < mean(y1) & results_ci[, 2] > mean(y1))*100,
  "mi_y2"=mean(results_ci[, 3] < mean(y2) & results_ci[, 4] > mean(y2))*100,
  "mi_y3"=mean(results_ci[, 5] < mean(y3) & results_ci[, 6] > mean(y3))*100) 
```

### Real dataset from section 7

# Literature
